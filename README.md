# Ollama cached models
This image contains the ollama tool packaged with two LLMs:
- [**Mistral**](https://ollama.com/library/mistral) for general chat (intruct variant)
- [**Stable-Code**](https://ollama.com/library/stable-code) for use with [llama-coder](https://github.com/ex3ndr/llama-coder) (stable-code:3b-code-q4_0 variant)

This combinaition is tailord to my use of the tool and fits my hardware specs. It has only been tested on a CUDA GPU.

If this project doesn't fit your needs, I'd encourage you to fork it and adapt it to yours.